# ğŸ›¡ï¸ Traductor AutomÃ¡tico de EspaÃ±ol a Quechua

Este proyecto implementa el entrenamiento de un modelo BART de arquitectura de 

Transformers con el fine tuning con un Corpus y el prototipo visual de utilizaciÃ³n del modelo.

Este repositorio forma parte del curso **Proyecto de InvestigaciÃ³n II (MIA 403)**.

---

## ğŸ‘¥ Autores
- Fernado AndrÃ©s Herrera Cubas â€“ [@nandex2122](https://github.com/nandex2122)
- JosÃ© Carlos Ramos Maldonado â€“ [@joseramos135](https://github.com/joseramos135)

---

## ğŸ“Š Dataset
- **Fuente**: [Hugging Face â€“ Spanish to Quechua translation](https://huggingface.co/somosnlp-hackathon-2022/)  
- **Registros**: 128.5K pÃ¡rrafos traducidos, 78.5MB.  
- **VersiÃ³n usada**: descargada el 28/10/2022  
- **Hash** (SHA256): `3b7e7fed69aeeabb5eb3802c4dd74e6166ddb0c5341f9ee7161068fbc821bc77`  

### Fuentes Adicionales

#### 1. Literatura - Paco Yunque
- **Fuente**: [Centro de Recursos - Paco Yunque EdiciÃ³n MultilingÃ¼e](https://centroderecursos.cultura.pe/es/registrobibliografico/paco-yunque-edici%C3%B3n-multiling%C3%BCe)
- **Registros**: 142 pares de oraciones
- **TamaÃ±o**: 16 KB
- **AÃ±o**: 2023
- **Tipo**: Literatura peruana (CÃ©sar Vallejo)

#### 2. Material Educativo - MINEDU
- **Fuente**: [Repositorio MINEDU](https://repositorio.minedu.gob.pe/handle/20.500.12799/10380)
- **Registros**: 471 pares de oraciones
- **TamaÃ±o**: 35 KB
- **AÃ±o**: 2014
- **Tipo**: Material educativo oficial

#### 3. Manual MÃ©dico
- **Fuente**: [Manual de SemiologÃ­a en Quechua](https://www.cmp.org.pe/wp-content/uploads/2020/07/ManualSemiologiaQuechua-2020.pdf)
- **Registros**: 310 pares de oraciones
- **TamaÃ±o**: 24 KB
- **Tipo**: TerminologÃ­a mÃ©dica y de salud

#### 4. Literatura - El Vencedor
- **Fuente**: [Centro de Recursos - El Vencedor EdiciÃ³n MultilingÃ¼e](https://centroderecursos.cultura.pe/es/registrobibliografico/el-vencedor-edici%C3%B3n-multiling%C3%BCe)
- **Registros**: 150 pares de oraciones
- **TamaÃ±o**: 25 KB
- **AÃ±o**: 2024
- **Tipo**: Literatura contemporÃ¡nea

#### 5. Corpus MÃ©dico RAG - EspaÃ±ol
- **Fuente**: Oraciones mÃ©dicas en idioma EspaÃ±ol generadas por GPT-4 para entrenamiento RAG
- **Registros**: 20,000 oraciones segmentadas
- **TamaÃ±o**: 1,977 KB
- **AÃ±o**: 2025
- **Especialidad**: TerminologÃ­a mÃ©dica, sÃ­ntomas, diagnÃ³sticos usados en centros de salud

---

## ğŸ—‚ï¸ Estructura del repositorio (Pendiente)
```
data/
 â”œâ”€â”€ raw/          # dataset original
 â”œâ”€â”€ interim/      # dataset procesado y unido
 â”œâ”€â”€ processed/    # dataset formato de Corpus paralelo
notebooks/
	 â”œâ”€â”€ EDA         
	 â”‚   â”œâ”€â”€ Evaluacion_KFold_BART.ipynb            # Notebook para la ejecucion del EDA realizado
	 â”œâ”€â”€ Entrenamiento         
	 â”‚   â”œâ”€â”€ 01_Generacion_Corpus.ipynb            # Notebook para la generaciÃ³n de corpus
	 â”‚   â”œâ”€â”€ 02_Entrenamiento_modelo_BART.ipynb    # Notebook para ejecutar BART en un servidor (para Quechua)
	 â”œâ”€â”€ Metrica         
	 â”‚   â”œâ”€â”€ Evaluacion_KFold_BART.ipynb            # Notebook para la ejecucion dde metricas BLEU y OUGE-L
src/               
 â”œâ”€â”€ Entrenamiento_modelo/               
 â”œ   â”œâ”€â”€ 01_procesamiento_data.py       		# script para la generaciÃ³n de corpus
 â”œ   â””â”€â”€ 02_entrenamiento_modelo_baseline.py    # script para ejecutar BART en un servidor (para Quechua)
 â””â”€â”€â”€Prototipo_usuario/               
	 â”œâ”€â”€ app/                              # Carpeta principal de la aplicaciÃ³n Flask
	 â”‚   â”œâ”€â”€ __pycache__/            	   # Archivos compilados automÃ¡ticamente por Python
	 â”‚   â”œâ”€â”€ static/               		   # Archivos estÃ¡ticos (imÃ¡genes, CSS, JS)  
	 â”‚   â”‚   â””â”€â”€ Logo_UNI.png			   # Imagen usada en la aplicaciÃ³n        
	 â”‚   â”œâ”€â”€ templates/    				   # Plantillas HTML de Flask          
	 â”‚   â”‚   â””â”€â”€ index.html     		   # PÃ¡gina principal con el formulario     
	 â”‚   â”œâ”€â”€ app.py              	       # Archivo principal Flask (servidor web)          
	 â”‚   â”œâ”€â”€ run_with_ngrok.py             # Script para ejecutar Flask + Ngrok
	 â”‚   â””â”€â”€ translator.py                 # Script que carga el modelo y hace la traducciÃ³n
	 â””â”€â”€ bart_traducido_final/             # Carpeta con el modelo fine-tuned de BART          

logs/              # archivos de logging y mÃ©tricas
slides/            # presentaciones de resultados
README.md
pyproject.toml
poetry.lock / requirements.txt
.gitignore
```

---

## âš™ï¸ Requisitos
Instalar dependencias usando [Poetry](https://python-poetry.org/):  
```bash
poetry install
```
O con `pip`:  
```bash
pip install -r requirements.txt
```

---

## ğŸš€ CÃ³mo ejecutar el pipeline
1. **Procesamiento Corpus**
   - OpciÃ³n A (script): 
   ```bash
   python src/Entrenamiento_modelo/01_procesamiento_data.py
   ```  
   - OpciÃ³n B (notebook): 
	- Abrir y ejecutar `notebooks/Entrenamiento_modelo/01_Generacion_Corpus.ipynb`
	
   - Ambos realizan la lectura de los diccionarios, limpieza, generaciÃ³n de traducciones en paralelo.  
   - Guardado en `data/processed/corpus_total_formateado.txt`.

2. **Entrenamiento baseline y Fine tuning**
   - OpciÃ³n A (script): 
   ```bash
   python src/Entrenamiento_modelo/02_entrenamiento_modelo_baseline.py
   ```
   - OpciÃ³n B (notebook):  
     - Abrir y ejecutar `notebooks/Entrenamiento_modelo/02_Entrenamiento_modelo_BART.ipynb`  
   - Ambos generan resultados en `logs/log_baseline.txt`

3. **Prototipo**
   
   Probar Prototipo Online [Click AquÃ­](https://0704e13dcc33.ngrok-free.app)

   ```bash
   ngrok config add-authtoken
   python src/Protipo_usuario/app/app.py
   src/Protipo_usuario/app/ngrok http 5000

   ```
   - Genera el servicio para levantar la url pÃºblica del prototipo
   - Generan resultados en `logs/log_baseline.txt`  

---
# ğŸ” Feature Engineering y ConfiguraciÃ³n del Modelo de TraducciÃ³n EspaÃ±olâ€“Quechua

Este documento describe el proceso de **fine-tuning del modelo BART** para la tarea de **traducciÃ³n automÃ¡tico EspaÃ±olâ€“Quechua**, desarrollado en Jupyter Notebook.  
Se presentan dos versiones del modelo: una entrenada con **2 Ã©pocas** (para validaciÃ³n inicial) y otra con **50 Ã©pocas** (para convergencia completa y evaluaciÃ³n final).

---

## ğŸ“„ 1. ConfiguraciÃ³n General

| ParÃ¡metro | Modelo 2 Ã‰pocas | Modelo 50 Ã‰pocas |
|------------|----------------|------------------|
| Modelo base | `facebook/bart-base` | `facebook/bart-base` |
| Tokenizer | `BartTokenizer` | `BartTokenizer` |
| Longitud mÃ¡xima (`max_length`) | 128 | 128 |
| TamaÃ±o de batch | 8 | 8 |
| Learning rate | 5e-5 | 5e-5 |
| Ã‰pocas | 2 | 50 |
| Tiempo estimado de entrenamiento | ~3.5 min | ~1.5 h |
| MÃ©tricas registradas | No registradas | No registradas |

---

## âš™ï¸ 2. Feature Engineering (FE) para Texto

El **preprocesamiento textual** aplicado antes del entrenamiento incluye:

1. **TokenizaciÃ³n** con `BartTokenizer`, aplicando segmentaciÃ³n sub-palabra (Byte-Pair Encoding).  
2. **ConversiÃ³n** de los pares EspaÃ±olâ€“Quechua a tensores (`input_ids`, `attention_mask`, `labels`).  
3. **Padding y truncado** hasta una longitud fija de 128 tokens.  
4. No se aplica limpieza adicional (stopwords, lematizaciÃ³n, etc.), ya que BART maneja ruido textual de forma eficiente.

> ğŸ§© Este pipeline representa un **FE estÃ¡ndar para modelos seq2seq**, optimizado para tareas de traducciÃ³n.

---

## ğŸ” 3. RecuperaciÃ³n de InformaciÃ³n (RAG)

El modelo actual **no implementa un mÃ³dulo de recuperaciÃ³n** (Retriever o RAG).

- **TF-IDF / BM25:** No aplican.  
- **PosiciÃ³n del pasaje:** No aplica.  
- **SeÃ±ales del retriever:** (score, overlap, top-k) No implementadas.

> ğŸ’¡ En futuras versiones se planea integrar un **mÃ³dulo RAG** que utilice bÃºsqueda semÃ¡ntica (TF-IDF o embeddings) para seleccionar pasajes relevantes antes de traducir.

---

## ğŸ§© 4. Longitud de Contexto

- Longitud mÃ¡xima: **128 tokens** por entrada.  
- Este valor equilibra costo computacional y capacidad de generalizaciÃ³n.  
- En escenarios de chatbot o RAG, se puede ampliar a **256â€“512 tokens** para mejorar cobertura contextual (a costa de mayor latencia).

---

## ğŸ§  5. Embeddings

- Los **embeddings son generados internamente** por el modelo BART durante el entrenamiento.  
- No se utilizan embeddings externos como Word2Vec o Sentence-BERT.  
- Esta decisiÃ³n reduce **latencia** y simplifica el **despliegue en producciÃ³n**.

---

## â±ï¸ 6. EstimaciÃ³n de Costo y Latencia

| ConfiguraciÃ³n | Tiempo estimado de entrenamiento | Uso esperado |
|----------------|----------------------------------|---------------|
| 2 Ã©pocas | ~3.5 minutos | ValidaciÃ³n funcional / test rÃ¡pido |
| 50 Ã©pocas | ~1.5 horas | Modelo final / despliegue |

*(Tiempos estimados en GPU NVIDIA T4 o RTX 3060, dataset de tamaÃ±o medio).*

---

## ğŸ“Š 7. Conclusiones

- El modelo de **2 Ã©pocas** permite validar el pipeline y estructura de datos.  
- El modelo de **50 Ã©pocas** logra una mejor convergencia y coherencia en la traducciÃ³n.  
- No se aplican tÃ©cnicas RAG ni embeddings externos.  
- La longitud de contexto de 128 tokens garantiza **eficiencia y bajo costo**.

---

## ğŸš€ 8. PrÃ³ximos Pasos

- Incorporar mÃ©tricas de evaluaciÃ³n (BLEU, ROUGE, chrF).  
- Registrar tiempos reales de entrenamiento e inferencia.  
- Implementar mÃ³dulo RAG con recuperaciÃ³n semÃ¡ntica.  

---
 
ğŸ“… **VersiÃ³n:** Octubre 2025  

---
## ğŸ“œ Licencia
Uso acadÃ©mico â€“ Universidad Nacional de IngenierÃ­a (UNI).
