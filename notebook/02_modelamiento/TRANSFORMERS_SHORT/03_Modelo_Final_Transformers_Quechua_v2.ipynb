{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6758767-43c0-4c17-97ed-fccef6f14c79",
   "metadata": {},
   "source": [
    "# Modelo Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedae30-9df9-4db0-823f-e2909adbcfa8",
   "metadata": {},
   "source": [
    "## Inicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b6dbb-dcb2-4437-88cb-05a6d4327eeb",
   "metadata": {},
   "source": [
    "### 1) Instalando librerias requeridas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf66b78f-fd4b-4a1e-a493-10e399b3408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.12/site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.53.0)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting sounddevice\n",
      "  Using cached sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.12/site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Installing collected packages: soxr, audioread, soundfile, sounddevice, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 pooch-1.8.2 sounddevice-0.5.2 soundfile-0.13.1 soxr-0.5.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio transformers librosa sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3428eca9-5257-4ac2-b082-94c1d9a3fab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6919539e-e311-4f3b-a78e-b4234461fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Using cached speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.12/site-packages (from SpeechRecognition) (4.12.2)\n",
      "Using cached speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.14.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323f618f-47af-4877-9d9b-d09c3f3d5afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sacremoses) (4.67.1)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30a1014-6a1d-40d4-9b5b-3ad4de25e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.12/site-packages (from gtts) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /opt/conda/lib/python3.12/site-packages (from gtts) (8.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2024.12.14)\n",
      "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gtts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "### 2) Implementando el modelo con Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73e4135cb9f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAX_SEQ_LEN = 128\n",
    "MAX_SEQ_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pos_embed_matrix.shape)\n",
    "#         print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) ## query\n",
    "        self.W_k = nn.Linear(d_model, d_model) ## Key\n",
    "        self.W_v = nn.Linear(d_model, d_model) ## value\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Decoder embedding and postional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "        return source_mask, target_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "### 3) Definiendo la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Archivo donde se guarda el diccionario descargado\n",
    "PATH = \"./01_corpus_total_formateado.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0af1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identificando el archivo, separado por espacio en blanco\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if line.count('\\t') == 3]\n",
    "##eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]\n",
    "##eng_spa_pairs = [line.strip().split('\\t') for line in lines if line.count('\\t') == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c930226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_spa_pairs=eng_spa_pairs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f3c7e9-51e8-49fb-bf7a-088913a462dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['000002',\n",
       "  'b) ¿Cómo lograban mantenerse estos siervos de tiempo completo?',\n",
       "  '000002',\n",
       "  '(2) ¿Imaynatam tukuy tiemponkuwan Diosta serviq punta cristianokunaqa mantienekuqku?'],\n",
       " ['000003',\n",
       "  'Cabe señalar que solo la Biblia explica de manera satisfactoria cuál es el origen de los muchos idiomas que conocemos hoy.',\n",
       "  '000003',\n",
       "  'Bibliallam allintapuni willawanchik imanasqam runakunaqa kunan tiempopi achka rimayniyoq kasqankuta.'],\n",
       " ['000004',\n",
       "  'Pero al mismo tiempo nos advierte que “el amor al dinero es raíz de toda clase de males ”.',\n",
       "  '000004',\n",
       "  'Ichaqa nintaqmi: “Qollqella kuyayqa tukuy mana allin ruraykunapa mamanmi ”, nispa.'],\n",
       " ['000005',\n",
       "  'Hoy, todo el que desea vivir de acuerdo con sus mandamientos considera que conmemorar el aniversario de la muerte de Cristo es de suma importancia.',\n",
       "  '000005',\n",
       "  'Kay tiempopipas Cristopa kamachisqanman hina kawsaqkunaqa ancha valorniyoqtam qawanku paypa wañukusqan punchaw yuyariytaqa.'],\n",
       " ['000006',\n",
       "  'RESPUESTA: El Reino de Dios es un gobierno celestial, y su Rey es Jesús.',\n",
       "  '000006',\n",
       "  'KUTICHIYNIN: Hanaq pachapi kaq huk gobiernom, kamachiqninñataqmi Jesus.'],\n",
       " ['000007', '¿CUÁL ES LA SOLUCIÓN?', '000007', '¿IMATAM RURACHWAN?'],\n",
       " ['000008',\n",
       "  'Adán y Eva desobedecieron a Dios, de modo que fueron expulsados del Edén.',\n",
       "  '000008',\n",
       "  'Adanwan Evaqa Jehová Diostam mana kasukurqakuchu, hinaspam Edenmanta qarqochikurqaku.'],\n",
       " ['000009',\n",
       "  'Sin embargo, poco después de escribir esa carta, los de la casa de Cloe le informaron de que en la congregación de Corinto había graves divisiones.',\n",
       "  '000009',\n",
       "  'Ichaqa chay carta qellqasqan qepamanmi Cloepa familian Pabloman willaykurqaku Corinto congregacionpi llumpay liryanakuy kasqanmanta.'],\n",
       " ['000010',\n",
       "  'Jesús cumplió de forma sorprendente esta profecía durante su ministerio.',\n",
       "  '000010',\n",
       "  'Jesusqa Isaiaspa nisqantam kay Pachapi Diospa munayninta ruraspan allinta cumplirqa.'],\n",
       " ['000011',\n",
       "  'Cierto diccionario bíblico lo define como “fortaleza espiritual, emocional y moral para hablar y actuar sin temor ante dificultades y peligros ” (The New Interpreter’s Dictionary of the Bible).',\n",
       "  '000011',\n",
       "  'Qarinchakuyqa llumpay sasachakuypi tarikuspa mana manchakuspa ruwaymi hinaspa mana manchakuspa rimaymi.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095f4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [pair[1] for pair in eng_spa_pairs]  ## índices de la ubicación del archivo descargado\n",
    "spa_sentences = [pair[3] for pair in eng_spa_pairs]  ## índices de la ubicación del archivo descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9e1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b) ¿Cómo lograban mantenerse estos siervos de tiempo completo?', 'Cabe señalar que solo la Biblia explica de manera satisfactoria cuál es el origen de los muchos idiomas que conocemos hoy.', 'Pero al mismo tiempo nos advierte que “el amor al dinero es raíz de toda clase de males ”.', 'Hoy, todo el que desea vivir de acuerdo con sus mandamientos considera que conmemorar el aniversario de la muerte de Cristo es de suma importancia.', 'RESPUESTA: El Reino de Dios es un gobierno celestial, y su Rey es Jesús.', '¿CUÁL ES LA SOLUCIÓN?', 'Adán y Eva desobedecieron a Dios, de modo que fueron expulsados del Edén.', 'Sin embargo, poco después de escribir esa carta, los de la casa de Cloe le informaron de que en la congregación de Corinto había graves divisiones.', 'Jesús cumplió de forma sorprendente esta profecía durante su ministerio.', 'Cierto diccionario bíblico lo define como “fortaleza espiritual, emocional y moral para hablar y actuar sin temor ante dificultades y peligros ” (The New Interpreter’s Dictionary of the Bible).']\n",
      "['(2) ¿Imaynatam tukuy tiemponkuwan Diosta serviq punta cristianokunaqa mantienekuqku?', 'Bibliallam allintapuni willawanchik imanasqam runakunaqa kunan tiempopi achka rimayniyoq kasqankuta.', 'Ichaqa nintaqmi: “Qollqella kuyayqa tukuy mana allin ruraykunapa mamanmi ”, nispa.', 'Kay tiempopipas Cristopa kamachisqanman hina kawsaqkunaqa ancha valorniyoqtam qawanku paypa wañukusqan punchaw yuyariytaqa.', 'KUTICHIYNIN: Hanaq pachapi kaq huk gobiernom, kamachiqninñataqmi Jesus.', '¿IMATAM RURACHWAN?', 'Adanwan Evaqa Jehová Diostam mana kasukurqakuchu, hinaspam Edenmanta qarqochikurqaku.', 'Ichaqa chay carta qellqasqan qepamanmi Cloepa familian Pabloman willaykurqaku Corinto congregacionpi llumpay liryanakuy kasqanmanta.', 'Jesusqa Isaiaspa nisqantam kay Pachapi Diospa munayninta ruraspan allinta cumplirqa.', 'Qarinchakuyqa llumpay sasachakuypi tarikuspa mana manchakuspa ruwaymi hinaspa mana manchakuspa rimaymi.']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d11478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpiando las tildes\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ac79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> imaynatam tukuy tiemponkuwan diosta serviq punta cristianokunaqa mantienekuqku <eos>',\n",
       " '<sos> bibliallam allintapuni willawanchik imanasqam runakunaqa kunan tiempopi achka rimayniyoq kasqankuta <eos>',\n",
       " '<sos> ichaqa nintaqmi qollqella kuyayqa tukuy mana allin ruraykunapa mamanmi nispa <eos>',\n",
       " '<sos> kay tiempopipas cristopa kamachisqanman hina kawsaqkunaqa ancha valorniyoqtam qawanku paypa wa ukusqan punchaw yuyariytaqa <eos>',\n",
       " '<sos> kutichiynin hanaq pachapi kaq huk gobiernom kamachiqnin ataqmi jesus <eos>',\n",
       " '<sos> imatam rurachwan <eos>',\n",
       " '<sos> adanwan evaqa jehova diostam mana kasukurqakuchu hinaspam edenmanta qarqochikurqaku <eos>',\n",
       " '<sos> ichaqa chay carta qellqasqan qepamanmi cloepa familian pabloman willaykurqaku corinto congregacionpi llumpay liryanakuy kasqanmanta <eos>',\n",
       " '<sos> jesusqa isaiaspa nisqantam kay pachapi diospa munayninta ruraspan allinta cumplirqa <eos>',\n",
       " '<sos> qarinchakuyqa llumpay sasachakuypi tarikuspa mana manchakuspa ruwaymi hinaspa mana manchakuspa rimaymi <eos>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ec85e-bd55-455d-ae0a-2697cc6e51a6",
   "metadata": {},
   "source": [
    "### 4) Diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97931cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diccionario (índices a palabras y palabras a índices)\n",
    "def build_vocab(sentences):\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    word_count = Counter(words)\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)  ## Ordenamiento de las palabras desc x nro. palabras\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    word2idx['<pad>'] = 1   ## índices español de la ubicación del archivo descargado\n",
    "    word2idx['<unk>'] = 3   ## índices ingles de la ubicación del archivo descargado\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa8738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "eng_vocab_size = len(eng_word2idx)  # Cantidad de enunciados\n",
    "spa_vocab_size = len(spa_word2idx)  # Cantidad de enunciados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79d6b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40710 105976\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)  ## Nro.de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e564017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clase Dataset traducción\n",
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        self.eng_sentences = eng_sentences  ## sentencia español\n",
    "        self.spa_sentences = spa_sentences  ## sentencia ingles \n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "        \n",
    "    def __len__(self):   ## tamaño del dataset\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):  ## indexar cada elemento del dataset\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        # return tokens idxs\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b579577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para el padding\n",
    "def collate_fn(batch):\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    #eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0) \n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=1) ## el padding viene de build_vocab\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=1)\n",
    "    return eng_batch, spa_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc45cc-506e-4a81-aecb-681031747181",
   "metadata": {},
   "source": [
    "### 5) Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d514b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamiento del modelo\n",
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            # Decoder preprocessing\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            # Zero grads\n",
    "            optimiser.zero_grad()\n",
    "            # run model\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            # loss\\\n",
    "            loss = loss_function(output, target_output)\n",
    "            # gradient and update parameters\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2379ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "##BATCH_SIZE = 32\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e08eef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\"\"\"\n",
    "model = Transformer(d_model=256, num_heads=8, d_ff=1024, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1181a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e265e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/80, Loss: 3.1442\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs = 80)\n",
    "##torch.save(model.state_dict(), \"mi_modelo.pt\")  ## solo guarda el diccionario\n",
    "torch.save(model, \"mi_modelo_quechua_v2.pt\")   ## guarda el diccionario y arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989b5ae-0f48-4d40-8589-8241e06f2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Guardando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fd511e2-5fb4-42b6-8e0d-236497c53be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Transformer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a0ed008-09c6-4808-b684-c27ebb0d6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "##torch.save(model.state_dict(), \"mi_modelo.pt\")  ## solo guarda el diccionario\n",
    "torch.save(model, \"mi_modelo_quechua.pt\")   ## guarda el diccionario y arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f86e5-4aec-4a32-9ce2-8a69aa76bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "## para cargar el modelo\n",
    "model.load_state_dict(torch.load(\"mi_modelo.pt\"))\n",
    "model.eval()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5a6b0-29b5-4bbb-a36b-5c121b5187aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    time.sleep(300)  # Espera 60 segundos (ajustable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58edf5-d9fb-4ea6-abf5-9e439a615d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
