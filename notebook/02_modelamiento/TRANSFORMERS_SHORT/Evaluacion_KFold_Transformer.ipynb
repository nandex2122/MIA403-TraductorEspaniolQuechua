{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71005d4c-ceba-4a4e-88eb-b78de8ae6ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (4.53.1)\n",
      "Requirement already satisfied: datasets in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (2.7.1+cpu)\n",
      "Requirement already satisfied: nltk in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: click in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\programdata\\anaconda3\\envs\\transformers_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2025.7.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ferhe\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "Successfully installed fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch nltk rouge-score pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e52b5d-1145-40fd-a3e8-294be8ab495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo de ejecución: cpu\n",
      "\n",
      "--- Verificación de rutas de archivos ---\n",
      "DEBUG: CSV_PATH = 'C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\datos\\corpus_trad.csv' - Existe: True\n",
      "DEBUG: MODEL_PATH_V1 = 'C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\modelos\\Transformers\\mi_modelo.pt' - Existe: True\n",
      "DEBUG: MODEL_PATH_V2 = 'C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\modelos\\Transformers\\mi_modelo_quechua_v2.pt' - Existe: True\n",
      "DEBUG: TOKENIZER_NAME_OR_PATH = 'facebook/bart-base' (Se cargará desde Hugging Face Hub)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "import torch\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "import os # Importar os para la verificación de rutas\n",
    "\n",
    "# Configuración de rutas\n",
    "CSV_PATH = r\"C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\datos\\corpus_trad.csv\"\n",
    "MODEL_PATH_V1 = r\"C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\modelos\\Transformers\\mi_modelo.pt\"\n",
    "MODEL_PATH_V2 = r\"C:\\DATA\\FERNANDOHC\\EDUCACION\\MAESTRIA\\UNI_MAI\\SEMESTRE_3\\MIA-204ProyectoDeInvestigacion1\\Proyecto_Traductor_Esp_Quechua\\modelos\\Transformers\\mi_modelo_quechua_v2.pt\"\n",
    "\n",
    "# --- CAMBIO CLAVE AQUÍ ---\n",
    "# Ya que no tienes la carpeta del tokenizer localmente, lo cargamos por su nombre\n",
    "# desde Hugging Face Hub. Es VITAL que este nombre corresponda al modelo base\n",
    "# con el que tus modelos .pt fueron entrenados (ej. \"facebook/bart-base\").\n",
    "TOKENIZER_NAME_OR_PATH = \"facebook/bart-base\" \n",
    "\n",
    "# Parámetros de evaluación\n",
    "NUM_SENTENCES = 50\n",
    "NUM_RUNS = 5\n",
    "\n",
    "# Configurar dispositivo para PyTorch (GPU si está disponible, sino CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo de ejecución: {device}\")\n",
    "\n",
    "# Ignorar warnings que puedan surgir de librerías\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Verificación de rutas de archivos (útil para depuración) ---\n",
    "print(f\"\\n--- Verificación de rutas de archivos ---\")\n",
    "print(f\"DEBUG: CSV_PATH = '{CSV_PATH}' - Existe: {os.path.exists(CSV_PATH)}\")\n",
    "print(f\"DEBUG: MODEL_PATH_V1 = '{MODEL_PATH_V1}' - Existe: {os.path.exists(MODEL_PATH_V1)}\")\n",
    "print(f\"DEBUG: MODEL_PATH_V2 = '{MODEL_PATH_V2}' - Existe: {os.path.exists(MODEL_PATH_V2)}\")\n",
    "print(f\"DEBUG: TOKENIZER_NAME_OR_PATH = '{TOKENIZER_NAME_OR_PATH}' (Se cargará desde Hugging Face Hub)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eb9eb6b-0eaf-4f5a-8f21-21cdaa4fae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando datos: 100%|██████████| 262855/262855 [00:34<00:00, 7678.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de pares válidos cargados: 262855\n",
      "Longitud promedio español: 117.1 caracteres\n",
      "Longitud promedio quechua: 120.1 caracteres\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data(csv_path):\n",
    "    \"\"\"\n",
    "    Carga un archivo CSV, lo procesa para extraer pares de texto\n",
    "    y lo convierte en un objeto Dataset de Hugging Face.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path,\n",
    "            usecols=['source', 'target'], # Solo nos interesan estas columnas\n",
    "            encoding='utf-8',\n",
    "            on_bad_lines='warn' # Advertir sobre líneas mal formadas en lugar de fallar\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el CSV: {e}\")\n",
    "        return Dataset.from_list([]) # Retornar un Dataset vacío si hay un error\n",
    "    \n",
    "    data_pairs = []\n",
    "    # Iterar sobre las filas del DataFrame para crear pares de datos\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Procesando datos\"):\n",
    "        # Asegurarse de que ambas columnas 'source' y 'target' no sean NaN\n",
    "        if pd.notna(row['source']) and pd.notna(row['target']):\n",
    "            data_pairs.append({\n",
    "                \"spanish\": str(row['source']).strip(), # Asegurar que es string y eliminar espacios\n",
    "                \"quechua\": str(row['target']).strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nTotal de pares válidos cargados: {len(data_pairs)}\")\n",
    "    if data_pairs:\n",
    "        # Calcular y mostrar la longitud promedio de las oraciones\n",
    "        avg_len_spanish = sum(len(p['spanish']) for p in data_pairs) / len(data_pairs)\n",
    "        avg_len_quechua = sum(len(p['quechua']) for p in data_pairs) / len(data_pairs)\n",
    "        print(f\"Longitud promedio español: {avg_len_spanish:.1f} caracteres\")\n",
    "        print(f\"Longitud promedio quechua: {avg_len_quechua:.1f} caracteres\")\n",
    "    \n",
    "    return Dataset.from_list(data_pairs) # Convertir la lista de diccionarios a un objeto Dataset\n",
    "\n",
    "# Cargar y preparar el dataset al inicio\n",
    "raw_dataset = load_and_prepare_data(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e63214-d6a5-4bf0-a323-f584f597a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calcula las métricas BLEU y ROUGE-L para un conjunto de predicciones y referencias.\n",
    "    \"\"\"\n",
    "    # BLEU Score: Mide la similitud n-grama entre la traducción y la referencia.\n",
    "    # nltk.translate.bleu_score.corpus_bleu espera:\n",
    "    # - references: Una lista de referencias, donde cada referencia es una lista de tokens (palabras).\n",
    "    #   Como solo tenemos una referencia por predicción, la envolvemos en otra lista: [[ref1_tokens], [ref2_tokens]].\n",
    "    # - predictions: Una lista de traducciones, donde cada traducción es una lista de tokens.\n",
    "    bleu = corpus_bleu([[ref.split()] for ref in references], \n",
    "                       [pred.split() for pred in predictions])\n",
    "    \n",
    "    # ROUGE-L Score: Mide la superposición de secuencias más largas comunes (LCS).\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        score = scorer.score(ref, pred)['rougeL'].fmeasure # Obtenemos el F-measure de ROUGE-L\n",
    "        rouge_scores.append(score)\n",
    "    rouge = np.mean(rouge_scores) # Calculamos la media de todos los scores ROUGE-L\n",
    "    \n",
    "    return {\"bleu\": bleu, \"rouge\": rouge}\n",
    "\n",
    "def load_local_model(model_path, tokenizer_name_or_path):\n",
    "    \"\"\"\n",
    "    Carga un modelo de PyTorch guardado localmente (.pt) y su tokenizador\n",
    "    desde Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    # Cargar tokenizer: AutoTokenizer detectará el tipo de tokenizador por su nombre\n",
    "    # o desde los archivos si se le pasa una ruta local.\n",
    "    # Aquí lo cargamos desde el nombre en el Hub (requiere internet la primera vez).\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "    \n",
    "    # Cargar configuración del modelo: Es esencial que la configuración coincida con\n",
    "    # la arquitectura del modelo que se entrenó y se guardó en .pt.\n",
    "    # Usamos el mismo nombre que para el tokenizer si son consistentes (como en BART).\n",
    "    config = AutoConfig.from_pretrained(tokenizer_name_or_path)\n",
    "    \n",
    "    # Crear una instancia del modelo con la configuración cargada\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config).to(device)\n",
    "    \n",
    "    # --- SOLUCIÓN AL UnpicklingError: Añadir weights_only=False ---\n",
    "    # Esto es necesario si el archivo .pt se guardó con una versión antigua de PyTorch\n",
    "    # o si contiene metadatos además de solo los pesos.\n",
    "    # Solo usar si confías en el origen del archivo.\n",
    "    state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Cargar los pesos (state_dict) en la instancia del modelo\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Poner el modelo en modo evaluación (desactiva dropout, batch norm, etc.)\n",
    "    model.eval() \n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def evaluate_model(model_path, dataset_subset, tokenizer_name_or_path):\n",
    "    \"\"\"\n",
    "    Evalúa un modelo de traducción en un subconjunto de datos.\n",
    "    Carga el modelo y el tokenizador, genera predicciones y calcula métricas.\n",
    "    \"\"\"\n",
    "    # Cargar el tokenizador y el modelo usando la función auxiliar\n",
    "    tokenizer, model = load_local_model(model_path, tokenizer_name_or_path)\n",
    "    \n",
    "    predictions = [] # Para almacenar las traducciones generadas por el modelo\n",
    "    references = []  # Para almacenar las traducciones de referencia (ground truth)\n",
    "\n",
    "    # Desactivar el cálculo de gradientes durante la inferencia para ahorrar memoria y tiempo\n",
    "    with torch.no_grad(): \n",
    "        for example in tqdm(dataset_subset, desc=\"Evaluando modelo\"):\n",
    "            # Tokenizar el texto de entrada (español)\n",
    "            inputs = tokenizer(\n",
    "                example[\"spanish\"], \n",
    "                return_tensors=\"pt\",       # Retornar tensores de PyTorch\n",
    "                max_length=128,            # Longitud máxima de secuencia de entrada\n",
    "                truncation=True,           # Truncar si excede max_length\n",
    "                padding=\"max_length\"       # Rellenar con padding hasta max_length\n",
    "            ).to(device) # Mover los inputs al dispositivo (GPU/CPU)\n",
    "            \n",
    "            # Generar la traducción\n",
    "            # num_beams=5: Usa beam search con 5 haces para una mejor calidad de generación\n",
    "            # early_stopping=True: Detiene la generación tan pronto como se encuentran las secuencias completas\n",
    "            output = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
    "            \n",
    "            # Decodificar la salida generada de IDs a texto legible\n",
    "            pred_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            predictions.append(pred_text)\n",
    "            references.append(example[\"quechua\"]) # Añadir la referencia (traducción real)\n",
    "            \n",
    "    # Liberar memoria de GPU después de la evaluación de este modelo\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache() # Limpiar la caché de la GPU\n",
    "\n",
    "    return calculate_metrics(predictions, references) # Retornar las métricas calculadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "145c8303-a803-4540-b65d-38ef3ecf9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Asegúrate de que esta variable esté definida antes de las clases\n",
    "MAX_SEQ_LEN = 64 # O el valor que hayas usado en tu entrenamiento original, que parece ser 64 o 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Definición de la clase PositionalEmbedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "# Definición de la clase MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model) ## query\n",
    "        self.W_k = nn.Linear(d_model, d_model) ## Key\n",
    "        self.W_v = nn.Linear(d_model, d_model) ## value\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "\n",
    "        return weighted_values, attention\n",
    "\n",
    "\n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "\n",
    "        return weighted_values, attention\n",
    "\n",
    "\n",
    "# Definición de la clase PositionFeedForward\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "# Definición de la clase EncoderSubLayer\n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "# Definición de la clase Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Definición de la clase DecoderSubLayer\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "\n",
    "# Definición de la clase Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Definición de la clase Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size,\n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # Encoder mask\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "\n",
    "        # Decoder embedding and postional encoding\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        # Decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "\n",
    "        return self.output_layer(output)\n",
    "\n",
    "\n",
    "    def mask(self, source, target):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "        # The subsequent mask (look-ahead mask)\n",
    "        size = target.size(1) # get seq_len for subsequent mask\n",
    "        no_peak_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "        no_peak_mask = torch.autograd.Variable(torch.from_numpy(no_peak_mask) == 0).to(device)\n",
    "        target_mask = target_mask & no_peak_mask\n",
    "        return source_mask, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d73230b5-2edc-473f-9ca9-53025943a182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejecución 1/5 ---\n",
      "Evaluando modelo baseline...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class '__main__.Transformer'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluando modelo baseline...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Llamar a la función evaluate_model con la ruta del modelo v1, el subconjunto de datos\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# y el nombre del tokenizador (que se cargará del Hub)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m metrics_v1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH_V1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOKENIZER_NAME_OR_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m bleu_scores_v1\u001b[38;5;241m.\u001b[39mappend(metrics_v1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbleu\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     20\u001b[0m rouge_scores_v1\u001b[38;5;241m.\u001b[39mappend(metrics_v1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[22], line 61\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_path, dataset_subset, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03mEvalúa un modelo de traducción en un subconjunto de datos.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mCarga el modelo y el tokenizador, genera predicciones y calcula métricas.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Cargar el tokenizador y el modelo usando la función auxiliar\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[43mload_local_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Para almacenar las traducciones generadas por el modelo\u001b[39;00m\n\u001b[0;32m     64\u001b[0m references \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Para almacenar las traducciones de referencia (ground truth)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 48\u001b[0m, in \u001b[0;36mload_local_model\u001b[1;34m(model_path, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Cargar los pesos (state_dict) en la instancia del modelo\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Poner el modelo en modo evaluación (desactiva dropout, batch norm, etc.)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\transformers_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2525\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m \n\u001b[0;32m   2490\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2522\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[1;32m-> 2525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2527\u001b[0m     )\n\u001b[0;32m   2529\u001b[0m missing_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2530\u001b[0m unexpected_keys: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class '__main__.Transformer'>."
     ]
    }
   ],
   "source": [
    "# Listas para almacenar las puntuaciones BLEU y ROUGE-L de cada ejecución para ambos modelos\n",
    "bleu_scores_v1, rouge_scores_v1 = [], []\n",
    "bleu_scores_v2, rouge_scores_v2 = [], []\n",
    "\n",
    "# Bucle para ejecutar la evaluación múltiples veces (NUM_RUNS)\n",
    "for run in range(NUM_RUNS):\n",
    "    print(f\"\\n--- Ejecución {run+1}/{NUM_RUNS} ---\")\n",
    "    \n",
    "    # Seleccionar un subconjunto aleatorio del dataset para esta ejecución\n",
    "    # raw_dataset.shuffle(seed=run): Baraja el dataset de forma reproducible para cada ejecución\n",
    "    # .select(range(NUM_SENTENCES)): Selecciona solo las primeras NUM_SENTENCES\n",
    "    small_dataset = raw_dataset.shuffle(seed=run).select(range(NUM_SENTENCES))\n",
    "    \n",
    "    # --- Evaluar modelo v1 (modelo baseline) ---\n",
    "    print(\"Evaluando modelo baseline...\")\n",
    "    # Llamar a la función evaluate_model con la ruta del modelo v1, el subconjunto de datos\n",
    "    # y el nombre del tokenizador (que se cargará del Hub)\n",
    "    metrics_v1 = evaluate_model(MODEL_PATH_V1, small_dataset, TOKENIZER_NAME_OR_PATH)\n",
    "    bleu_scores_v1.append(metrics_v1[\"bleu\"])\n",
    "    rouge_scores_v1.append(metrics_v1[\"rouge\"])\n",
    "    \n",
    "    # --- Evaluar modelo v2 (modelo refinado) ---\n",
    "    print(\"Evaluando modelo refinado...\")\n",
    "    metrics_v2 = evaluate_model(MODEL_PATH_V2, small_dataset, TOKENIZER_NAME_OR_PATH)\n",
    "    bleu_scores_v2.append(metrics_v2[\"bleu\"])\n",
    "    rouge_scores_v2.append(metrics_v2[\"rouge\"])\n",
    "    \n",
    "    # Mostrar resultados parciales para la ejecución actual\n",
    "    print(f\"\\nResultados ejecución {run+1}:\")\n",
    "    print(f\"BLEU - v1: {metrics_v1['bleu']:.4f} | v2: {metrics_v2['bleu']:.4f}\")\n",
    "    print(f\"ROUGE-L - v1: {metrics_v1['rouge']:.4f} | v2: {metrics_v2['rouge']:.4f}\")\n",
    "\n",
    "# (Opcional) Mostrar resultados finales promediados después de todas las ejecuciones\n",
    "print(\"\\n--- Resultados finales promedio ---\")\n",
    "print(f\"BLEU promedio - v1: {np.mean(bleu_scores_v1):.4f} | v2: {np.mean(bleu_scores_v2):.4f}\")\n",
    "print(f\"ROUGE-L promedio - v1: {np.mean(rouge_scores_v1):.4f} | v2: {np.mean(rouge_scores_v2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f51be8-232a-4e75-ad9d-bcb85e6847ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180ea97-9dd5-4438-8589-79801de287c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa80ed-3c3c-4519-86f2-2ee4e614d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Cálculo de estadísticas\n",
    "# Medias\n",
    "mean_bleu_v1 = np.mean(bleu_scores_v1)\n",
    "mean_bleu_v2 = np.mean(bleu_scores_v2)\n",
    "mean_rouge_v1 = np.mean(rouge_scores_v1)\n",
    "mean_rouge_v2 = np.mean(rouge_scores_v2)\n",
    "\n",
    "# Desviaciones estándar\n",
    "std_bleu_v1 = np.std(bleu_scores_v1)\n",
    "std_bleu_v2 = np.std(bleu_scores_v2)\n",
    "std_rouge_v1 = np.std(rouge_scores_v1)\n",
    "std_rouge_v2 = np.std(rouge_scores_v2)\n",
    "\n",
    "# Diferencias\n",
    "delta_bleu = mean_bleu_v2 - mean_bleu_v1\n",
    "delta_rouge = mean_rouge_v2 - mean_rouge_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd4315-9279-4ba7-af3b-44e560315cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6: Visualización de resultados\n",
    "# Tabla de resultados por fold\n",
    "table_html = \"\"\"\n",
    "<h3>Resultados Comparativos por Fold</h3>\n",
    "<table style=\"border-collapse: collapse; width: 100%;\">\n",
    "<tr style=\"background-color: #f2f2f2;\">\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Fold</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">BLEU Baseline</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">BLEU Refinado</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Δ BLEU</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">ROUGE-L Baseline</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">ROUGE-L Refinado</th>\n",
    "<th style=\"border: 1px solid #ddd; padding: 8px; text-align: left;\">Δ ROUGE-L</th>\n",
    "</tr>\n",
    "\"\"\"\n",
    "\n",
    "for run in range(NUM_RUNS):\n",
    "    delta_b = bleu_scores_v2[run]-bleu_scores_v1[run]\n",
    "    delta_r = rouge_scores_v2[run]-rouge_scores_v1[run]\n",
    "    \n",
    "    table_html += f\"\"\"\n",
    "<tr>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{run+1}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{bleu_scores_v1[run]:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{bleu_scores_v2[run]:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px; color: {'green' if delta_b >=0 else 'red'}\">{delta_b:+.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{rouge_scores_v1[run]:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{rouge_scores_v2[run]:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px; color: {'green' if delta_r >=0 else 'red'}\">{delta_r:+.4f}</td>\n",
    "</tr>\n",
    "\"\"\"\n",
    "\n",
    "# Estadísticas finales\n",
    "table_html += f\"\"\"\n",
    "<tr style=\"font-weight: bold; background-color: #e6f3ff;\">\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">Media ±STD</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{mean_bleu_v1:.4f} ±{std_bleu_v1:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{mean_bleu_v2:.4f} ±{std_bleu_v2:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px; color: {'green' if delta_bleu >=0 else 'red'}\">{delta_bleu:+.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{mean_rouge_v1:.4f} ±{std_rouge_v1:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px;\">{mean_rouge_v2:.4f} ±{std_rouge_v2:.4f}</td>\n",
    "<td style=\"border: 1px solid #ddd; padding: 8px; color: {'green' if delta_rouge >=0 else 'red'}\">{delta_rouge:+.4f}</td>\n",
    "</tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0fd82-4115-4a4d-9f13-b9f6e00fbab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7: Ejemplos de traducción\n",
    "print(\"\\n=== Ejemplos de Traducción (última ejecución) ===\")\n",
    "\n",
    "# Cargar modelos para ejemplos\n",
    "tokenizer, model_v1 = load_local_model(MODEL_PATH_V1, TOKENIZER_PATH)\n",
    "_, model_v2 = load_local_model(MODEL_PATH_V2, TOKENIZER_PATH)\n",
    "\n",
    "for i in range(min(3, NUM_SENTENCES)):\n",
    "    example = small_dataset[i]\n",
    "    \n",
    "    # Generar traducciones\n",
    "    inputs = tokenizer(example[\"spanish\"], return_tensors=\"pt\", \n",
    "                     max_length=128, truncation=True, padding=\"max_length\").to(device)\n",
    "    \n",
    "    # Modelo v1\n",
    "    output_v1 = model_v1.generate(**inputs, max_length=128)\n",
    "    pred_v1 = tokenizer.decode(output_v1[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Modelo v2\n",
    "    output_v2 = model_v2.generate(**inputs, max_length=128)\n",
    "    pred_v2 = tokenizer.decode(output_v2[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nEjemplo {i+1}:\")\n",
    "    print(f\"Español: {example['spanish']}\")\n",
    "    print(f\"Referencia (Quechua): {example['quechua']}\")\n",
    "    print(f\"Baseline: {pred_v1}\")\n",
    "    print(f\"Refinado: {pred_v2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
